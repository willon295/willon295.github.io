(window.webpackJsonp=window.webpackJsonp||[]).push([[205],{590:function(n,e,t){"use strict";t.r(e);var o=t(13),a=Object(o.a)({},(function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[t("h1",{attrs:{id:"环境准备"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#环境准备"}},[n._v("#")]),n._v(" 环境准备")]),n._v(" "),t("ol",[t("li",[n._v("创建 maven 项目，添加依赖")])]),n._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[n._v("    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n            <version>2.8.2</version>\n        </dependency>\n\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-hdfs-client</artifactId>\n            <version>2.8.2</version>\n        </dependency>\n    </dependencies>\n")])])]),t("ol",{attrs:{start:"2"}},[t("li",[n._v("将 "),t("code",[n._v("core-site.xml")]),n._v(" , "),t("code",[n._v("hdfs-site.xml")]),n._v(" 复制到项目 "),t("code",[n._v("resource")]),n._v("目录")])]),n._v(" "),t("h1",{attrs:{id:"coding"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#coding"}},[n._v("#")]),n._v(" Coding")]),n._v(" "),t("p",[n._v("加载所有需要导入的类都是 "),t("code",[n._v("org.apache.hadoop.*")])]),n._v(" "),t("h2",{attrs:{id:"简单的几个例子"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#简单的几个例子"}},[n._v("#")]),n._v(" 简单的几个例子")]),n._v(" "),t("p",[n._v("上传下载文件，压缩上下载文件，查看文件内容")]),n._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[n._v('package cn.willon.hadoop.hdfs;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.*;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.compress.CompressionCodec;\nimport org.apache.hadoop.io.compress.CompressionCodecFactory;\nimport org.apache.hadoop.io.compress.CompressionInputStream;\nimport org.apache.hadoop.io.compress.CompressionOutputStream;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\n\npublic class Client  {\n\n    /**\n     * 将本地文件上传\n     *\n     * @param localInput 本地文件\n     * @param remotePath hdfs文件路径\n     * @throws IOException 文件读写异常\n     */\n    public void putLocalFileToHDFS(Path localInput, Path remotePath) throws IOException {\n\n        //1. 获取配置文件\n        Configuration configuration = new Configuration();\n\n        //2. 获取集群文件系统\n        FileSystem remoteFS = FileSystem.get(configuration);\n\n        //3. 获取本地文件系统\n        LocalFileSystem localFS = FileSystem.getLocal(configuration);\n\n\n        //4. 通过remoteFS 获取文件输出流\n        FSDataOutputStream fsDataOutputStream = remoteFS.create(remotePath);\n\n        //5. 通过 localFS 获取文件输入流\n        FSDataInputStream fsDataInputStream = localFS.open(localInput);\n\n\t\t//通过 IOUtils将数据流对接\n        IOUtils.copyBytes(fsDataInputStream, fsDataOutputStream, 1024, true);\n        localFS.close();\n        remoteFS.close();\n    }\n\n\n    /**\n     * 查看HDFS文件内容\n     * @param filePath 文件路径\n     * @throws IOException 读取文件异常\n     */\n    public  void  catHDFSFileContent(String  filePath) throws IOException {\n        String  line  ;\n        Path  path = new Path(filePath);\n\n        Configuration conf = new Configuration();\n\n        FileSystem f = FileSystem.get(conf);\n\n        FSDataInputStream open = f.open(path);\n\n        BufferedReader b = new BufferedReader(new InputStreamReader(open));\n        while (( line  = b.readLine()) != null){\n            System.out.println(line);\n\n        }\n\n    }\n\n\n    /**\n     * 获取 HDFS文件 到本地\n     * @param remotePath HDFS文件\n     * @param localPath 本地文件\n     * @throws IOException 文件读写异常\n     */\n    public  void  getHDFSFileToLocal(String remotePath , String localPath) throws IOException {\n\n        Configuration conf = new Configuration();\n        FileSystem rfs = FileSystem.get(conf);\n        LocalFileSystem lfs = FileSystem.getLocal(conf);\n\n        FSDataInputStream ris = rfs.open(new Path(remotePath));\n        FSDataOutputStream los = lfs.create(new Path(localPath));\n\n        IOUtils.copyBytes(ris,los,1024,true);\n        ris.close();\n        los.close();\n    }\n\n\n    /**\n     * 将本地文件进行压缩，上传至 HDFS\n     * @param localFile 本地文件\n     * @param remoteFile 存放到 HDFS的文件如： hadoop.gz\n     * @throws IOException 文件读写异常\n     */\n    public  void  putFileByCompressor(String localFile,String remoteFile) throws IOException {\n\n\n        Configuration configuration  = new Configuration();\n\n        //获取 远程和本地文件系统\n        FileSystem rfs = FileSystem.get(configuration);\n        LocalFileSystem lfs  = FileSystem.getLocal(configuration);\n\n        //创建文件输出流 ， 本地文件读取流\n        FSDataOutputStream fos = rfs.create(new Path(remoteFile));\n        FSDataInputStream fis = lfs.open(new Path(localFile));\n\n\n        //创建压缩工厂对象\n        CompressionCodecFactory factory = new CompressionCodecFactory(configuration);\n\n        //工厂对象通过 文件名 获取相应的压缩格式  的压缩对象\n        CompressionCodec codec = factory.getCodec(new Path(remoteFile));\n\n\n        //压缩对象创建压缩的文件输出流\n        CompressionOutputStream cos = codec.createOutputStream(fos);\n\n        //通过工具类 将文件惊醒读写\n        IOUtils.copyBytes(fis,cos,1024,true);\n\n\n        fis.close();\n        fos.close();\n        cos.close();\n        lfs.close();\n        rfs.close();\n    }\n\n\n    /**\n     * 将HDFS压缩文件读取到本地\n     * @param remotePath 远程文件\n     * @param localFile 本地文件\n     * @throws IOException 文件读写异常\n     */\n    public  void   getCompressedFileToLocal(String remotePath,String  localFile) throws IOException {\n\n\n        Configuration  configuration = new Configuration();\n\n        //获取 远程和本地文件系统\n        FileSystem rfs = FileSystem.get(configuration);\n        LocalFileSystem lfs  = FileSystem.getLocal(configuration);\n\n        FSDataInputStream rfis = rfs.open(new Path(remotePath));\n        FSDataOutputStream lfos = lfs.create(new Path(localFile));\n\n        CompressionCodecFactory factory = new CompressionCodecFactory(configuration);\n        CompressionCodec codec = factory.getCodec(new Path(remotePath));\n        //读取压缩文件，创建输出流，输出流是解压缩后的流\n        CompressionInputStream cis = codec.createInputStream(rfis);\n        IOUtils.copyBytes(cis,lfos,1024,true);\n\n\n    }\n\n    public static void main(String[] args) throws IOException {\n\n         Client c = new Client();\n        // c.putLocalFileToHDFS(new Path("/home/willon/tmp/num.txt"),new Path("/num.txt"));\n\n        //c.getHDFSFileToLocal("/num.txt","/home/willon/num.txt");\n        c.getCompressedFileToLocal(" /20180310/p0010_cite/input/hadoop.gz","/home/willon/hadoop.txt");\n    }\n}\n\n')])])])])}),[],!1,null,null,null);e.default=a.exports}}]);