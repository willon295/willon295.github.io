(window.webpackJsonp=window.webpackJsonp||[]).push([[201],{584:function(e,t,a){"use strict";a.r(t);var n=a(13),o=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[e._v("在代码编写前，先引入依赖")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.8.2</version>\n        </dependency>\n    </dependencies>\n")])])]),a("h1",{attrs:{id:"coding"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#coding"}},[e._v("#")]),e._v(" CODING")]),e._v(" "),a("ol",[a("li",[a("code",[e._v("WcMapper")])])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('package wc;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * 参数解释：\n * KEYIN: 读取的数据的偏移量数据类型\n * VALUEIN： 读取的数据的数据类型\n * KEYOUT： 输出给框架的返回值的KEY的数据类型\n * VALUEOUT： 输出给框架的返回只的 VALUE 的数据类型\n * 注意; 由于java自带的数据类型序列化比较臃肿，所以hadoop采用了自己的包装类\n * TEXT--String\n * IntWritable--Integer\n */\npublic class WcMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n\n    /**\n     * mapper方法只负责提取有用的信息\n     *\n     * @param key     读取数据的偏移量，一般不需要处理\n     * @param value   读取完的一行内容，这里是文本类型的数据\n     * @param context 上下文对象，用于将数据返回给框架\n     */\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        //将数据转换成String类型\n        String line = value.toString();\n\n        //将内容进行切割\n        String[] words = line.split(" ");\n\n        //把读取到的每个单词 作为 key，value为 1\n        for (String word : words\n                ) {\n            context.write(new Text(word), new IntWritable(1));\n        }\n\n    }\n}\n\n')])])]),a("ol",{attrs:{start:"2"}},[a("li",[a("code",[e._v("WcReducer")])])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("package wc;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;\n\n\n/**\n * 参数解释：\n * KEYIN: mapper阶段设置的 KEY 数据类型\n * VALUEIN： mapper阶段设置的 VALUE 数据类型\n * KEYOUT： 输出给框架的返回值的 KEY 的数据类型\n * VALUEOUT： 输出给框架的返回只的 VALUE 的数据类型\n */\npublic class WcReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n\n    /**\n     * 负责将一组结果进行处理，汇总 处理完一组之后，继续下一组\n     * 收到的数据是mapper之后经过shuffle处理的一组数据,比如：<hello,<1,1,1>>\n     *\n     * @param key     比如：<hello,<1,1,1>>中的 `hello`\n     * @param values  比如：<hello,<1,1,1>>中的`<1,1,1>`\n     * @param context 上下文对象\n     */\n    @Override\n    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n        int count = 0;\n        for (IntWritable value : values\n                ) {\n            //将 `<1,1,1,1,1>遍历相加\n            count += value.get();\n        }\n\n        //将处理完的数据返回给框架这里是 `<hello,5>`\n        context.write(key, new IntWritable(count));\n\n    }\n}\n")])])]),a("ol",{attrs:{start:"3"}},[a("li",[a("code",[e._v("WcJobSubmmiter")])])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("package wc;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n\n/**\n * 其实是一个 yarn 的客户端\n * 功能： 将jar 文件提交给yarn ，yarn再分发到服务器\n */\npublic class WcJobSubmiter {\n\n    public static void main(String[] args) throws Exception {\n\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        job.setJarByClass(WcJobSubmiter.class);\n\n        //设置 Mapper 和 Reducer 的类\n        job.setMapperClass(WcMapper.class);\n        job.setReducerClass(WcReducer.class);\n\n\n        //设置 Mapper 输出的数据类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        //设置 Reducer 输出的数据类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        //设置 输入的 数据类型 和 路径\n        job.setInputFormatClass(TextInputFormat.class);\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n\n\n        //设置 输出的 数据类型 和 路径\n        job.setOutputFormatClass(TextOutputFormat.class);\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        boolean res = job.waitForCompletion(true);\n        System.exit(res ? 0 : 1);\n\n    }\n}\n")])])]),a("ol",{attrs:{start:"4"}},[a("li",[e._v("打包 "),a("code",[e._v("mvn package")]),e._v(", 生成的jar 文件为改名为 "),a("code",[e._v("wc.jar")])])]),e._v(" "),a("h1",{attrs:{id:"run"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#run"}},[e._v("#")]),e._v(" RUN")]),e._v(" "),a("ol",[a("li",[e._v("随便找一台 "),a("code",[e._v("非sourceManager")]),e._v(" 的机器，上传 "),a("code",[e._v("wc.jar")]),e._v(",")]),e._v(" "),a("li",[e._v("输入")])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("hadoop jar wc.jar wc.WcJobSubmiter  /aa.txt  /wc/output\n")])])]),a("ol",{attrs:{start:"3"}},[a("li",[e._v("查看结果 "),a("code",[e._v("hadoop fs -cat /wc/output/part-r-00000")])])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("aaaa    1\ndfe     1\neee     2\neeerr   1\nhello   2\nlll     1\nme      1\noooe    1\nsss     1\nworld   1\nwwe     1\nwww     1\n")])])])])}),[],!1,null,null,null);t.default=o.exports}}]);