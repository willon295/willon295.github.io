(window.webpackJsonp=window.webpackJsonp||[]).push([[202],{585:function(n,o,t){"use strict";t.r(o);var e=t(13),a=Object(e.a)({},(function(){var n=this,o=n.$createElement,t=n._self._c||o;return t("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[t("p",[n._v("比如现在有一个文件")]),n._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[n._v("用户手机号  用户IP 用户访问网站  用户上行流量  用户下行流量\n13840583165    106.92.75.115\tbaidu.com\t240 34\n13826241303    106.85.238.220\tjd.com\t342\t543\n13852308726    123.233.137.34\trb.com\t564\t3432\n.......\n")])])]),t("p",[n._v("要用mapreduce统计用户的上行流量和下行流量和总流量")]),n._v(" "),t("h1",{attrs:{id:"分析"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#分析"}},[n._v("#")]),n._v(" 分析")]),n._v(" "),t("ol",[t("li",[n._v("用户手机号为 KEY")]),n._v(" "),t("li",[n._v("上行，下行，总和为 VALUE")]),n._v(" "),t("li",[n._v("行，下行，总和封装成一个自定义类")]),n._v(" "),t("li",[n._v("形如 "),t("code",[n._v("<13840583165, FlowBean@u45eer>")])])]),n._v(" "),t("h1",{attrs:{id:"实现"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#实现"}},[n._v("#")]),n._v(" 实现")]),n._v(" "),t("ol",[t("li",[n._v("自定义类")])]),n._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[n._v('package flowsum;\n\nimport org.apache.hadoop.io.Writable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n\n/**\n * 功能: 封装自定义的 VALUE 数据\n * 注意：\n *      1. 实现 `hadoop` 序列化接口 `Writable`\n *      2. 重写的方法内，属性的顺序要一样\n *      3. 重写 `toString()` 方法\n *      4. 必须要有空参构造函数\n */\npublic class FlowBean implements Writable {\n    private long upFlow;\n    private long downFlow;\n    private long sumFlow;\n\n    public FlowBean() {\n    }\n\n    public FlowBean(long upFlow, long downFlow) {\n        this.upFlow = upFlow;\n        this.downFlow = downFlow;\n        this.sumFlow = downFlow + upFlow;\n    }\n\n    public long getUpFlow() {\n        return upFlow;\n    }\n\n    public void setUpFlow(long upFlow) {\n        this.upFlow = upFlow;\n    }\n\n    public long getDownFlow() {\n        return downFlow;\n    }\n\n    public void setDownFlow(long downFlow) {\n        this.downFlow = downFlow;\n    }\n\n    public long getSumFlow() {\n        return sumFlow;\n    }\n\n    public void setSumFlow(long sumFlow) {\n        this.sumFlow = sumFlow;\n    }\n\n\n    /**\n     * @param out 向外输出序列化数据\n     */\n    public void write(DataOutput out) throws IOException {\n        out.writeLong(upFlow);\n        out.writeLong(downFlow);\n        out.writeLong(sumFlow);\n    }\n\n    /**\n     * @param in 从外部读取的序列化数据，用于反射生成类\n     */\n    public void readFields(DataInput in) throws IOException {\n        this.upFlow = in.readLong();\n        this.downFlow = in.readLong();\n        this.sumFlow = in.readLong();\n    }\n\n    @Override\n    public String toString() {\n        return upFlow+" "+downFlow+" "+sumFlow;\n    }\n}\n')])])]),t("ol",{attrs:{start:"2"}},[t("li",[n._v("FlowMapper")])]),n._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[n._v('package flowsum;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class FlowMapper extends Mapper<LongWritable, Text, Text, FlowBean> {\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        String line = value.toString();\n        String[] users = line.split("\\t");\n\n        long u = Long.parseLong(users[users.length - 1]);\n        long d = Long.parseLong(users[users.length - 2]);\n\n        context.write(new Text(users[0]), new FlowBean(u, d));\n\n    }\n}\n')])])]),t("ol",{attrs:{start:"3"}},[t("li",[n._v("FlowReducer")])]),n._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[n._v("package flowsum;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\npublic class FlowReducer extends Reducer<Text, FlowBean, Text, FlowBean> {\n    @Override\n    protected void reduce(Text key, Iterable<FlowBean> values, Context context) throws IOException, InterruptedException {\n\n        long u = 0;\n        long d = 0;\n        long s = 0;\n        for (FlowBean flow : values\n                ) {\n            d += flow.getDownFlow();\n            u += flow.getUpFlow();\n            s = flow.getSumFlow();\n        }\n        context.write(key,new FlowBean(u,d));\n    }\n}\n")])])]),t("ol",{attrs:{start:"4"}},[t("li",[n._v("FlowJobSubmitter")])]),n._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[n._v("package flowsum;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n\npublic class FlowJobSubmitter {\n\n    public static void main(String[] args) throws Exception {\n\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        job.setJarByClass(FlowJobSubmitter.class);\n\n        //设置 Mapper 和 Reducer 的类\n        job.setMapperClass(FlowMapper.class);\n        job.setReducerClass(FlowReducer.class);\n\n\n        //设置 Mapper 输出的数据类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(FlowBean.class);\n\n        //设置 Reducer 输出的数据类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n\n        //设置 输入的 数据类型 和 路径\n        job.setInputFormatClass(TextInputFormat.class);\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n\n\n        //设置 输出的 数据类型 和 路径\n        job.setOutputFormatClass(TextOutputFormat.class);\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        boolean res = job.waitForCompletion(true);\n        System.exit(res ? 0 : 1);\n\n    }\n}\n")])])])])}),[],!1,null,null,null);o.default=a.exports}}]);