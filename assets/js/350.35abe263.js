(window.webpackJsonp=window.webpackJsonp||[]).push([[350],{371:function(t,e,s){"use strict";s.r(e);var a=s(13),n=Object(a.a)({},(function(){var t=this,e=t.$createElement,s=t._self._c||e;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"安装"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#安装"}},[t._v("#")]),t._v(" 安装")]),t._v(" "),s("ol",[s("li",[t._v("安装Python3-pip")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("sudo    apt-get   install  python3-pip\n")])])]),s("ol",{attrs:{start:"2"}},[s("li",[t._v("安装scrapy")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("sudo pip3 install  scrapy\n")])])]),s("ol",{attrs:{start:"3"}},[s("li",[t._v("创建一个项目")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("scrapy startproject  myDemo\n")])])]),s("ol",{attrs:{start:"4"}},[s("li",[t._v("定义一个简单的item")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("# -*- coding: utf-8 -*-\n\nimport scrapy\n\nclass OccupationItem(scrapy.Item):\n    #定义要爬取的字段\n    occupation_name = scrapy.Field()\n    occupation_type = scrapy.Field()\n    occupation_number = scrapy.Field()\n    occupation_address = scrapy.Field()\n    publish_time = scrapy.Field()\n")])])]),s("ol",{attrs:{start:"5"}},[s("li",[t._v("定义一个spider，进入项目后创建")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("#基本语法\nscrapy genspider spider_name [spider_url]\n")])])]),s("p",[t._v("下面是一个spider的代码")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("# -*- coding: utf-8 -*-\nimport scrapy\nfrom mm.items import OccupationItem\nclass OccupationSpider(scrapy.Spider):\n    name = 'tencent_occupation'\n    allowed_domains = ['hr.tencent.com']\n    base_url = \"http://hr.tencent.com/position.php?keywords=&lid=0&start=\"\n    offset = 0\n    start_urls = [base_url + str(offset)]\n\n    def parse(self, response):\n        item = OccupationItem()\n        nodelist = response.xpath(\"//tr[@class='odd'] | //tr[@class='even']\")\n\n        for node in nodelist:\n            item['occupation_name'] = node.xpath(\"td[1]/a/text()\")[0].extract()\n\n            if len(node.xpath(\"td[2]/text()\")):\n                item['occupation_type'] = node.xpath(\"td[2]/text()\")[0].extract()\n            else:\n                item['occupation_type'] = \"\"\n            item['occupation_number'] = node.xpath(\"td[3]/text()\")[0].extract()\n            item['occupation_address'] = node.xpath(\"td[4]/text()\")[0].extract()\n            item['publish_time'] = node.xpath(\"td[5]/text()\")[0].extract()\n            yield item\n        if self.offset < 2210:\n            self.offset += 10\n            url = self.base_url + str(self.offset)\n            yield scrapy.Request(url=url, callback=self.parse)\n\n")])])]),s("ol",{attrs:{start:"6"}},[s("li",[t._v("定义管道文件，保存数据")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v('# -*- coding: utf-8 -*-\nimport json\n\n\nclass OccupationPipeline(object):\n    def __init__(self):\n        self.f = open("occupation.json", "w")\n        self.f.write("[\\n")\n\n    def process_item(self, item, spider):\n        content = json.dumps(dict(item), ensure_ascii=False) + ",\\n"\n        self.f.write(content)\n        return item\n\n    def close_spider(self, spider):\n        self.f.write("]")\n        self.f.close()\n')])])]),s("ol",{attrs:{start:"7"}},[s("li",[t._v("在配置文件"),s("code",[t._v("settings.py")]),t._v("中启用管道")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("ITEM_PIPELINES = {\n   'mm.pipelines.OccupationPipeline': 300,\n}\n")])])]),s("ol",{attrs:{start:"8"}},[s("li",[t._v("执行爬虫")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("#注意名字是spider里面定义的那个\nscrapy  crawl tencent_occupation\n")])])])])}),[],!1,null,null,null);e.default=n.exports}}]);